---
title: "german_credit"
author: "Michele Pesce"
date: "14 maggio 2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, eval=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gdata)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(readtext)
library(graphics)
library(caret)
library(randomForest)
#Retrieving the row original dataset 
credit_original <- read.csv("german.data", header=FALSE, sep = " ")
#Metadata definition and assignment
colNames = c("checking_account", "credit_duration", "Credit_history", "purpose", "credit_amount","savings_account","employment_since","percentage_income","personal_status","other_guarantors","residence","property","age","other_plans","housing","existing_credits","job","house_manteinant","telephone","foreign_worker","credit_response")
names(credit_original)   <-  colNames
```

## Oveview

The project is based on the study of the dataset **German credit data**, it has been downloaded from the UCI machine learning repository at **<https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29>**  It consists of a data text file containing 1k records and twenty attributes and has been converted in a dataframe named *credit_original*. At a first sight there are factors and numbers, all the factors are coded data as “Axxx” format whose meanings are  explained in a description text file  “german.doc” located at the same repository; from the same file have been extracted the metadata and promptly converted in variables names. In the next section the activity of conversion/replacement of dataset is shown.
This project relies on a typical binary classification problem: there is only a label “credit” whose possible values are “good” and “bad” which refers to a bank customer who asked for a credit.
The bank board would wish to know in advance who would be good an who would be bad, on the basis of customer attributes. The target is to study and  evalute best approaches to predict with high reliability the customer category on the base of attributes/factors after data manipulation/trasformation and a set of analysis tasks. 
This document reports the following sections:

- **Data cleaning** In this section,  the data integrity is explored, and possibly executed all the operations to make the dataset assessable for further analisys

- **Data exploration** Variables in the dataset are explored and checked in order to choose the relevant factors that could be used as predictors for the target labels.

- **The modeling approach** ML algorithms that best fit the kind of problem are chosen to predict the outcome, on the basis of the predictors assesed on the data exploration section. Usually, more than an ML model is checked and tuned and chosen the one that best fits the business requirement.

- **Results** The ML outcomes are compared, and evaluated pros and cons of the different approaches.

- **Conclusions** To summary the operations, and take in consideration further different approaches that could improve the project and the results



## Data Cleaning

The first operation that have to be done on data is to check the presence of the inconsistent values that could affect the correctness of analysis and bias the ML model results. Generally, if we don’t take in count incoherent values, mean and sd could be affected by bias, all the further analisys would be affected by some kind of error, and prediction would fail. First of all, the presence of NA’s must be checked. Fortunately, the data frame does not contain NA’s values as shown in the following R code,
so all the metrics are calculated on true data.


```{r dataCleaning, eval=TRUE, message=FALSE, warning=FALSE}

#exploring NA'S

sum(is.na(credit_original))
sum(complete.cases(credit_original))
glimpse(credit_original)
```

The next activity arranges data to get them more meaningful. The *credit_original* and the *key_map* dataframes are used as input for computing a new dataframe *credit_clear* whose values replace the coded "Axxx" values of  the original one.
The *key_map* is  extracted from the *german.doc*, conveniently slightly handly modified, that is a kind of hash-map with two variables: the keys as "Axxx" format and the values as meaningful descriptions. Next, the glimpse of   *credit_clear* df.


```{r dataReplacement, echo=FALSE, message=FALSE, warning=FALSE}

####@@@@REPLACING coded values with meaningful data 
library(tm)
#file.rename("german.doc","german.txt") #renames file to get it readable
key_map <- readLines("german.txt")
key_map <- key_map[grepl("A[0-9]", key_map)] #filter rows with coded variable
key_map <- as.data.frame(key_map)
key_map <- key_map %>% separate(key_map, into= c("KEY","VALUE"), sep=":" ) 
#in the next line of code the key_map DF is cleaned
key_map$KEY <- gsub(" ", "",key_map$KEY, fixed = TRUE) %>% gsub("\t", "",., fixed = TRUE)
key_map$VALUE <- gsub(" ", "",key_map$VALUE, fixed = TRUE) %>% gsub("\t", "",., fixed = TRUE)

credit_clear <- credit_original #copy original dataset in a new for replacing coded values
for (i in 1:length(names(credit_clear))) { #iterates all columns
  if (is.factor(credit_clear[,i]))#selects only columns with AXXX factor format
    credit_clear[,i] <- key_map$VALUE[match(credit_clear[,i], key_map$KEY)] #replace key with values
}

credit_clear <- credit_clear %>% mutate(credit_response=ifelse(.$credit_response==1,"good", "bad")) #making credit variable as categorical


```
## Data exploration

Prior to solve any ML problem, domain knowledge is essential, otherwise we will end up applying random algorithms and techniques blindly which may not give the right results. As a first step, to get a broad idea of what kind of data we are dealing with and summarize what has happened in the past, *descriptive analytics* will be applied.  The different attributes of the data will be observed in order to extract meaningful features, use statistics and visualizations to understand what has already happened. The information obtained could be  useful to select some predictive variables and exclude others that are not  The following table shows the number of unique values for each variable
```{r unique_values, echo=FALSE, message=FALSE, warning=FALSE}

library(miscset)
unique_values <- data_frame(VARIABLE=colNames[1],UNIQUE_VAL=nunique(credit_original[,1])) #Creating first row oa tibble
for (i in 2:length(names(credit_original))) { #calculate unique values for each variable and store them in the tibble adding rows
  unique_values <- bind_rows(unique_values,data_frame(VARIABLE=colNames[i],UNIQUE_VAL=nunique(credit_original[,i])))
}
unique_values %>% knitr::kable()



```

The *credit_amount*, *credit_duration*, *age* variables are the numerical ones: the first  concerns the credit required, it shows the most variability among all variables, the second about the credit_duration in weeks, the third is about the wide range of customer's age. The others are all categorical variables.

The dataset shows that "Good"" customers have `r  {length(which(credit_clear$credit_response=="good"))/10}`% proportion, and bad  `r {length(which(credit_clear$credit_response=="bad"))/10}`%. This is a first simple hint of predicted proportion

The next plot shows the *credit amount* factor vs credit response, split in two parts: the good credit  and the bad credit.
As shown, the credit response does not seem to rely on credit amount, the plots look like quite similar. Most of applicants ask for low amount, i.e. the 75% is lower than `r  {quantile(credit_clear$credit_amount, names=FALSE)[4]}` DM and the average is `r  {mean(credit_clear$credit_amount)}` DM

```{r credit_amount, echo=FALSE, message=FALSE, warning=FALSE}

#credit amount analysis in two separate plots, the first for the good and the second for the bad credit
#y limits chosen empirically to show real proportion
p1 <- credit_clear  %>% filter(credit_response=="good") %>% ggplot(aes(x=.$credit_amount)) +
  ggtitle("Good credit")+ geom_histogram(fill="green", bins=30)  +
  scale_y_continuous(limits=c(0,150))+xlab("Credit amount")+ ylab("Count") 

p2 <- credit_clear %>% filter(credit_response=="bad") %>%  ggplot(aes(x=.$credit_amount)) +
  ggtitle("Bad Crediit")+ geom_histogram(fill="red", bins=30)  +
  scale_y_continuous(limits=c(0,150))+ xlab("Credit amount")+ ylab("Count") 

library(gridExtra)
grid.arrange(p1, p2, ncol = 2)


```

The following illustrations show graphically and as-a-table the *Credit_history* distribution and proportions. More than half are trusted customers who are reliable debtors, one-third have critical account so that they need a loan for some purchase.
```{r credit_history_summary, echo=FALSE, message=FALSE, warning=FALSE}
#credit history distribution
credit_clear  %>% ggplot(aes(Credit_history)) +
  ggtitle(" Credit history distribution")+ geom_bar(fill="blue") + 
  xlab("Credit history") + ylab("Count") + theme(axis.text.x = element_text(angle = 90))
#the next code computes a table showing summarized categories and proportions inside Credit_history
as.data.frame(credit_clear %>% group_by(Credit_history) %>% summarize(amount=n())) %>% mutate(perc=amount/dim(credit_clear)[1]*100)%>% knitr::kable()


```

The next plot show no direct relationship between *Credit_history* and credit response ( the *credit_response* variable chosen as label). The two bar diagrams are quite similar and the different heights are releted to the proportion as shown in the table
```{r credit_history_by_credit_response, echo=FALSE, message=FALSE, warning=FALSE}

#credit history distribution vs credit response
p1 <- credit_clear  %>% filter(credit_response=="good") %>% ggplot(aes(x=.$Credit_history)) +
  ggtitle(" Good credit")+ geom_bar(fill="green") + 
  scale_y_continuous(limits=c(0,400))+xlab("Credit history")+ ylab("Count") + theme(axis.text.x = element_text(angle = 90))

p2 <- credit_clear %>% filter(credit_response=="bad") %>%  ggplot(aes(x=.$Credit_history)) +
  ggtitle(" Bad credit")+ geom_bar(fill="red") + 
  scale_y_continuous(limits=c(0,400))+xlab("Credit history")+ ylab("Count") + theme(axis.text.x = element_text(angle = 90))

library(gridExtra)
grid.arrange(p1, p2, ncol = 2)


```

The analysis next step focuses on *credit_amount*, *personal_status* and *age*. The table shows the proportion of personal status categories: more than half are male singles, the 30% are generally female. The majority of the first have bad credit, although `r  {credit_clear %>% filter(credit_amount >=7500   & personal_status=="male_single" & credit_response=="good") %>% count(.)   %>% .$n}` in 1000 is good for credit over 7500 DM as shown in the first plot.
The plot age vs personal status does not seem of particular interest, but generally the proportion of females that has "good"" credit is higher than "bad".
```{r credit_amount_by_personal_stat, echo=FALSE, message=FALSE, warning=FALSE}

credit_clear  %>%  ggplot(aes(x=personal_status,y=credit_amount,fill=credit_response)) +
  ggtitle(" Credit amount by personal status")+ geom_boxplot(varwidth = TRUE) + 
   theme(axis.text.x = element_text(angle = 90, hjust = 1))

#the next code computes a table showing summarized categories and proportions inside personal_status
as.data.frame(credit_clear %>% group_by(personal_status) %>% summarize(amount=n())) %>% 
  mutate(perc=amount/dim(credit_clear)[1]*100)%>% knitr::kable()

credit_clear  %>%  ggplot(aes(x=personal_status,y=age,fill=credit_response)) +
  ggtitle("Age by personal status")+ geom_boxplot(varwidth = TRUE) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

Prior to start job and employment-history, two tables have been provided in order to observe proportions. It is very hard for unskilled and uemployed applicants to ask for loan, infact only 22 in 1000 are present as observation, so that these have been cut out from box-plot analysis.
The data set top job is the skilled-employee, who more easly is a good credit candidate for middle/high credit amount. In proportion, managers/self-employed ask for high credit, but generally they are not considered very reliable. No way for unemployed, eventually skilled emplyees with consolidated employment history are favourite.
```{r emplyment-since, echo=FALSE, message=FALSE, warning=FALSE}
#Job Proportion table
as.data.frame(credit_clear %>% group_by(job) %>% summarize(Total=n())) %>%  mutate(perc=Total/dim(credit_clear)[1]*100)%>% knitr::kable()

#Employment history table
as.data.frame(credit_clear %>% group_by(employment_since) %>% summarize(Total=n())) %>% mutate(perc=Total/dim(credit_clear)[1]*100)%>% knitr::kable()

#Emplyment history vs credit_amount on job as parameter
#Unemployed has been kept out, because, as a parameter, is quite irrelevant
credit_clear  %>%  filter(job!="unemployed/unskilled_non-resident") %>% ggplot(aes(x=employment_since,y=credit_amount,fill=credit_response)) +
  ggtitle("emplyment-history by credit amount on job parameter")+ geom_boxplot(varwidth = TRUE) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  facet_grid(. ~ job)
```

The credit-history is important to figure out the customer trust; The 53% of them are reliable applicants that pay duly existing credits; the 30% are customers with critical account either with other credits in different bank. Anyway, as shown in the table, except for a 6% that has a good bank account, the proportions are fairly partitioned among enough account, negative account, or even with no account.
The *credit duration density plot* shows that most of good credits last few weeks, whilst long time credits tend to be not so good.
Looking at the *Cheking-account VS credit_duration* plot, except for customers with good account, the bank consider "bad" the credit_duration over 25 weeks: i.e. debtors,thin, and  no_accounts are absolutely bad, while good account is trusted as expected. The plot with credit_history parameter shows that existing_paid_duly customers are the most reliable.
Finally, in the last plot *Credit amount by Cheking-account*  high credit_amount is approved for some with no account, but generally -if not with a good account- bad credit response is prevalent.
```{r checking_account/credit_duration/history, echo=FALSE, message=FALSE, warning=FALSE}
#Credit_history proportion table
as.data.frame(credit_clear %>% group_by(Credit_history) %>% summarize(Total=n())) %>% 
  mutate(perc=Total/dim(credit_clear)[1]*100)%>% knitr::kable()

#checking_account proportion table
as.data.frame(credit_clear %>% group_by(checking_account) %>% summarize(Total=n())) %>% 
  mutate(perc=Total/dim(credit_clear)[1]*100)%>% knitr::kable()

#Duration density plot 
ggplot(credit_clear, aes(credit_duration, fill=credit_response)) + 
  geom_density(alpha=.5)

#Cheking-account VS credit_duration
credit_clear %>% 
  ggplot(aes(checking_account, credit_duration, fill=credit_response)) +
  ggtitle("Cheking-account by duration" )+
  geom_boxplot(varwidth = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Cheking-account VS credit_duration on credit_history parameter 
credit_clear %>% 
  ggplot(aes(checking_account, credit_duration, fill=credit_response)) +
  ggtitle("Cheking-account by duration on credit_history parameter")+
  geom_boxplot(varwidth = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(. ~ Credit_history)

#Cheking-account VS credit_amount
credit_clear %>% 
  ggplot(aes(checking_account, credit_amount, fill=credit_response)) +
  ggtitle("Credit amount by Cheking-account") +
  geom_boxplot(varwidth = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

As analysis last step, other meaningful attributes vs credit_amount have been illustrated. The saving_accounts proportion table shows that poors are the absolute majority: it is quite normal that who has low savings ( and possibly no other neither little property) tends to ask for a loan.
The 60% of total ask loans for cars or radio/television.
In the *Credit amount vs Property on saving-account parameter* the "excellent" saving_account value has been excluded, because it is a very few amount, also "good" and "very good" have very few applicants; poor or no-savings with no-property/unkown have favour but not for high amount.
The *Credit amount vs age on saving-account parameter* plot shows a different perspective based on age: generally the most of customers are in the 20-45 age rank, anyway who has poor or no-saving could be older. Only a little proportion of poor-saving-customer gets credit over 7500 DM.On the other hand, customers with no or unknown savings have high chance to get positive classification. Seemingly, as savings increase, the proportion of good evaluation  increases too regardless age;  anyway the correlation between savings and credit amount is `r  {cor(credit_clear$credit_amount,as.numeric(as.factor(credit_clear$savings_account))) }` which is too low and it would depend, among others, on different age and saving proportions.
```{r other/credit_amount/pt1, echo=FALSE, message=FALSE, warning=FALSE}
#savings_account proportion table
as.data.frame(credit_clear %>% group_by(savings_account) %>% summarize(Total=n())) %>% 
  mutate(perc=Total/dim(credit_clear)[1]*100)%>% knitr::kable()

#property proportion table
as.data.frame(credit_clear %>% group_by(property) %>% summarize(Total=n())) %>% 
  mutate(perc=Total/dim(credit_clear)[1]*100)%>% knitr::kable()

#purpose proportion table
as.data.frame(credit_clear %>% group_by(purpose) %>% summarize(Total=n())) %>% 
  mutate(perc=Total/dim(credit_clear)[1]*100)%>% knitr::kable()

#property VS credit_amount based on saving account
credit_clear %>% filter(savings_account!="excellent") %>%
  ggplot(aes(property, credit_amount, fill=credit_response)) +
  ggtitle("Credit amount vs Property on saving-account parameter") +
  geom_boxplot(varwidth = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(. ~ savings_account)

#age VS credit_amount based on saving amount
credit_clear %>% 
  ggplot(aes(age, credit_amount, color=credit_response)) +
  ggtitle("Credit amount vs age on saving-account parameter") +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(. ~ savings_account)






```

The *purpose VS credit_amount* plot shows that only for a very few cases it is possible to get a credit over 7500 DM just for radio/television or a new car; over  5000 for a used  car, but never over 7500. The data are very heterogeneous, so It is very difficult to observe a general behaviour.
The *guarantors VS credit_amount* does not help more, generally is better not to have other guarantors.
Looking at the *housing VS credit_amount* plot it is easy to observe that who pays the rent would not ask for high credit, similarly the owner that would pay for mortgage (who would not, probably could ask for high loan).
The favourite is a "for_free" customer that does not pay any rent or mortgage, even though high credit could not be ammitted.


```{r other/credit_amount/pt2, echo=FALSE, message=FALSE, warning=FALSE}

#purpose VS credit_amount
credit_clear %>% 
  ggplot(aes(purpose, credit_amount, fill=credit_response)) +
  ggtitle("purpose VS credit_amount") +
  geom_boxplot(varwidth = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
  
  
#guarantors VS credit_amount 
credit_clear %>% 
  ggplot(aes(other_guarantors, credit_amount, fill=credit_response)) +
  ggtitle("guarantors VS credit_amount") +
  geom_boxplot(varwidth = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
  
#housing VS credit_amount 
credit_clear %>% 
  ggplot(aes(housing, credit_amount, fill=credit_response)) +
  ggtitle("housing VS credit_amount") +
  geom_boxplot(varwidth = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
   

```

## The modeling approach

**Preprocessing**

At the end of data exploration, it’s important to inspect the relationship between variables, if two or more of them are correlated, they would give quite same information about prediction, so we should choose only the ones with no redundant information.
To apply the correlation function, The *credit_original* must be converted in df numeric, i.e. all the factor variable are transformed in numeric, so we have a *credit_num* dataframe as shown in a glimpse.
The correlation illustrated graphics show correlation between variables ordered by correlation rate. That couple that appears most correlated is *duration/credit_amount* whose value is `r  {cor(credit_clear$credit_duration,credit_clear$credit_amount) }`. Next, the related scatterplot that includes smoothing function; as shown, there is a natural straight relationship between amount and duration, the great is the credit the more are instalment (i.e. longer duration) when the credit is bad the tendency looks like quite different; anyway the general tendency seems to be too approximate, so that both should be considered as independent factors.

```{r Preproc_correlation, echo=FALSE, message=FALSE, warning=FALSE}
#converting credit_original to a numeric df
credit_num<- credit_original%>% mutate_if(is.factor, as.numeric) 
glimpse(credit_num)

library(corrplot)
#create a visual map showing the correlation values between all variables
#and is ordered by clustering
cor(credit_num) %>% corrplot(., type="upper", order="hclust", tl.col="black")


#the most correlated variables scatter plot and smoothing
credit_clear  %>% ggplot(aes(credit_amount,credit_duration, color=credit_response)) +geom_point() + geom_smooth()


```

To  further inspect if some variable could be cut out from predictors, caret package *nzv* function is applied. Next, the table that shows detail of function' s computation and the the  'nzv' `r  { nearZeroVar(credit_num, names=TRUE) }` variable that has to be excluded.
The dataframe credit_clean, from which the nzv  has been removed, is the final dataset to use for modeling approach, while the normalized *credit_num* dataset is used as input for PC Analysis.



```{r Preproc_nearZero, echo=FALSE, message=FALSE, warning=FALSE }

nzv <- NULL #initialize variable
#the nzv function looks for variables that could be cut out from predictors
nearZeroVar(credit_num, names=TRUE, saveMetrics = TRUE) #returns a detailed table
nzv <- nearZeroVar(credit_num, names=FALSE) #returns the column index
if(!is.null(nzv)) #if nzv is consistent ( calculated by function)..
  credit_clear <- credit_clear[,-nzv]#..cut nrz variable from df 

```

The goal of *normalization* is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. It is required only when features have different ranges. In the german credit dataset all the numeric variables are scaled and centered, while the factors have levels, that are numerical representations of classes.

```{r Preproc_normalize, echo=FALSE, message=FALSE, warning=FALSE}

credit_norm <- credit_clear # a df copy to be normalized 
col_num <- credit_norm[, sapply(credit_norm,is.numeric)] %>% names(.) #extracts numeric column names
col_cat <- credit_norm[, sapply(credit_norm,is.character)] %>% names(.) #extracts categorical column names

for (n in col_num) { #scales all numeric variables
  credit_norm[,n] <- scale(credit_norm[,n])
  
}
for (f in col_cat) { #factorizes all categorical variables
  credit_norm[,f] <- as.factor(credit_norm[,f])
}

```

The last step before applying ML models, is to partition the preprocessed dataset: the 70% of the entire datset as train set and the 30% for test set;  the train has been split in   the vector outcome (the *credit_response* variable as *y*)  and the nineteen predictor variables as *x*


```{r Preproc_dataPartition, echo=FALSE, message=FALSE, warning=FALSE}

set.seed(1)
test_index <- createDataPartition(y = credit_norm$credit_response, times = 1, p = 0.3, list = FALSE)
german_credit_train <- credit_norm[-test_index,] #70% of total
german_credit_test <- credit_norm[test_index,] #30% of total
x <- german_credit_train[,1:19] #predictors
y <- german_credit_train[,20] #outcome

```

**ML models** 

The following Machine learning algorithms have been chosen to fit train data and predict outcome on test data. These are among the most used, and fit quite well with the dataset; on the other hand the *knn* does not because it is not particularly effective on variables where there is a luck of hierarchical relationship between levels or numbers, so it has not been chosen as model.  

-	**GLM**
-	**Random Forest**
-	**Rpart**
-	**Naive bayes**

To explore the variable reduction approach, in the end of section the PCA model has been also inspected. 


**GLM**

This model works by trying to find out the relationship between the class variable and the other independent feature variables by estimating probabilities. Glm does not predict classes directly but the probability of the
outcome, so that for the metrics evaluation the probability is converted in outcome classes. In this case, since this is  a binary classification problem, the approach is by binomial logistic regression. This model does not require cross validation because there are not parameters to tune.

```{r model_GLM,  echo=FALSE, message=FALSE, warning=FALSE}
 #Fit the model on the german_credit_train training set.
#No cross validation, for GLM no tuning parameters
library(stats)
formula <-as.formula((names(x) %>% paste(.,collapse="+") %>% c("y",.) %>% paste0(. ,collapse="~")))
set.seed(1234)
fit_glm <- glm(formula, data=german_credit_train, family="binomial")
glm_pred_prob <- predict(fit_glm,german_credit_test, type="response") #Prediction as probability

glm_pred_prob <- round(glm_pred_prob)
glm_pred <- factor(ifelse(glm_pred_prob > 0.5, "good", "bad"))#Prediction as factor
cm_glm <- confusionMatrix(data=glm_pred, reference=german_credit_test$credit_response,positive = "good")
#Saving the model result
tab.glm <- data_frame(method = "GLM", Accuracy = cm_glm$overall["Accuracy"], Sensitivity=data.frame(cm_glm$byClass["Sensitivity"])[1,1],
           Specificity=data.frame(cm_glm$byClass["Specificity"])[1,1])

print( knitr::kable(tab.glm))

```


The table above shows the summarized metrics: the accuracy is quite good but not excellent. The sensitivity is good, the FPR=`r {1-cm_glm$byClass["Specificity"]}` is quite considerable.



**RANDOM FOREST**

Random forest is  a machine learning algorithm that comes from the family of ensemble learning algorithms. At any point in time,each tree in the ensemble of decision trees is built from a bootstrap sample, which is
basically sampling with replacement. The randomness of the algorithm increases the bias of the model slightly but decreases the variance and then prevents overfitting. The first approach takes into account all the variables, the second only a subset of them chosen after variable importance computed by cross-validated training 


```{r model_RF_train, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}

# RF Algorithm Cross Validation to look for mtry best parameter
metric <- "Accuracy"
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(123)
#COMMENT NEXT LINE if: rf.train <- readRDS("rfTrain.RDS")
#rf.train <- train(x,y, data=german_credit_train, method="rf",metric=metric, tuneLength=15, trControl=control)

rf.train <- readRDS("rfTrain.RDS") # UNCOMMENT IF RUN THE ABOVE LINE
plot(rf.train, main="Random forest cross-validation") #plots the accuracy/mtree graphic
rf.best_mtry <- rf.train$bestTune$mtry #model best tune

set.seed(123)
rf.fits.all <- randomForest(x, y,  ntree = 500, mtry=rf.best_mtry ) #fits using best tuning
rf.pred.all <- predict(rf.fits.all, german_credit_test) #predict with all factors
rf.cm <- confusionMatrix(rf.pred.all,german_credit_test$credit_response,positive = "good") #saving confusion matrix result
#Creating a metrics table
tab.rf <- data_frame(method = "RF", Accuracy = rf.cm$overall["Accuracy"], Sensitivity=data.frame(rf.cm$byClass["Sensitivity"])[1,1],
                                 Specificity=data.frame(rf.cm$byClass["Specificity"])[1,1])  
print(knitr::kable(tab.rf))


#fitting using the most important variable for the rf model
rf.importance <- varImp(rf.train) %>% .$importance 
rf.importance <- rf.importance%>% mutate(variable=row.names(rf.importance)) %>%  filter(., Overall >25) %>% .$variable
plot(varImp(rf.train), main="Random forest model: variable importance")

#select the first n factors( ten in this case) concat names by "+" and "~" 
#and finally transform it to use as first pred/ouput parameter in train function  
rf.formula <-as.formula(rf.importance %>% paste(.,collapse="+") %>% c("y",.) %>% paste0(. ,collapse="~"))
set.seed(123)
rf.fits.imp <- randomForest(rf.formula, data=german_credit_train, ntree = 500, mtry=rf.best_mtry )  #training model with the subset importance variable
rf.pred.imp <- predict(rf.fits.imp,german_credit_test)   #test data prediction 
rf.cm.imp <- confusionMatrix(rf.pred.imp, reference=german_credit_test$credit_response,positive = "good")
#Saving the model result
tab.rf_imp <- data_frame(method = "RF_IMP", Accuracy = rf.cm.imp$overall["Accuracy"], Sensitivity=data.frame(rf.cm.imp$byClass["Sensitivity"])[1,1],
           Specificity=data.frame(rf.cm.imp$byClass["Specificity"])[1,1]) 



```

The model, after cross-validation, is trained by randomForest randomForest's library function to tune both ntree and randomly selected predictors. The metrcs obtained with all predictors is better than the GLM apporach. The accuracy (0.80) is satisfactory and sensitivity too considering that the 70% of true data are *"good"*,  the specificity has improved too.
In the second RF model, the nine most important predictors have been chosen for training: except a slightly specificity improvement, the other metrics got worse!`r {print(knitr::kable(tab.rf_imp))}`



**RPART**

Rpart is an algorithm  that is a part of the decision trees family, that is CART family. Decision trees are mainly used for making decisions that would be most useful in reaching some objective and designing a strategy based on these decisions. They  are easy to represent, construct, and understand. However, the drawback is that they are very prone to overfitting. 
As shown in the metrics table, when using all variables as predictors, the results are not so good as RandomForest





```{r model_RPart,  echo=FALSE, message=FALSE, warning=FALSE}

set.seed(1234)
#rpart.train <- train(x, y,  #COMMENT IF: rpart.train <- readRDS("rpartTrain.RDS")
                    # method = 'rpart',
                    # metric = 'Accuracy',
                    # tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)))


rpart.train <- readRDS("rpartTrain.RDS") # UNCOMMENT IF RUN THE ABOVE LINE 
plot(rpart.train, main="Rpart cross-validation")

rpart.best_mtry <- rpart.train$bestTune$cp #model best tune for rpart (cp parameter)


rpart.pred.all<- predict(rpart.train,german_credit_test)   #test data prediction 

rpart.cm <- confusionMatrix(rpart.pred.all, reference=german_credit_test$credit_response,positive = "good")
#Saving the model result
tab.rpart <- data_frame(method = "RPART", Accuracy = rpart.cm$overall["Accuracy"], Sensitivity=data.frame(rpart.cm$byClass["Sensitivity"])[1,1],
           Specificity=data.frame(rpart.cm$byClass["Specificity"])[1,1]) 
#rpart_pred <- predict(train_rpart,german_credit_test, type = "prob")  %>% .$good #test data prediction for ROC curve

print(knitr::kable(tab.rpart))

#varable importance analysis
plot(varImp(rpart.train), main="Rpart model: variable importance")
#select variables whose importance is greater than zero and arrange them in a formula
rpart.imp <- varImp(rpart.train) %>% .$importance 
rpart.imp <- rpart.imp%>% mutate(variable=row.names(rpart.imp)) %>%  filter(., Overall >0) %>% .$variable

rpart.impIndexes <- names(x) %in% rpart.imp
rpart.dataImp <- x[rpart.impIndexes]

set.seed(1234)
rpart.fits.imp <- train(rpart.dataImp, y, 
                     method = 'rpart',
                     metric = 'Accuracy',
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25))
)


rpart.pred.imp <- predict(rpart.fits.imp,german_credit_test)   #test data prediction 
rpart.cm.imp <- confusionMatrix(rpart.pred.imp, reference=german_credit_test$credit_response,positive = "good")
#Saving the model result
tab.rpart_imp <- data_frame(method = "RPART_IMP", Accuracy = rpart.cm.imp$overall["Accuracy"], Sensitivity=data.frame(rpart.cm.imp$byClass["Sensitivity"])[1,1],
           Specificity=data.frame(rpart.cm.imp$byClass["Specificity"])[1,1])


```


The VarImp results are clear: the Rpart algorithm, for fitting, takes into account mainly six over nineteen variables; anyway the prediction after the retraining done with the six-variables subset of predictors, gives improved accuracy and sensitivity, but a very low  specificity, so that there is an high False positive rate. 
`r {print(knitr::kable(tab.rpart_imp))}`


**NAIVE BAYES**

The Naïve Bayes classifier is a simple probabilistic classifier which is based on Bayes theorem but with strong assumptions regarding independence.
Bayesian probability incorporates the concept of conditional probability, the probabilty of event A given that event B has occurred.
This approach is based on the processing of the following formula:

  $P(C_k \vert X) = \frac{P(C_k) \cdot P(X \vert C_k)}{P(X)}$
  
  Where:
  
  •	$P(C_k)$ = the *prior* probability of the outcome Y. 70% "good" and 30% "bad"
  
  •	$P(X)$ = the probability of the predictor variables (same as $P(C_k \vert x_1, \dots, x_p)$ )
             is the probability of each observed combination of predictor variables.
  
  •	$P(X \vert C_k))$ = the conditional probability or *likelihood*. Essentially, for each class of                         the response variable (i.e. "good" or "bad"), what is the probability of                            observing the predictor values.
  
  •	$P(C_k \vert X)$ = posterior probability. By combining our observed information, we are updating                        our *a priori* information on probabilities to compute a posterior                                  probability that an observation has class $C_k$
  
  The NB approach using all predictors gives good results, not the best accuracy, an acceptable  sensitivity and the best specificity so far. 
  
  
```{r model_NaiveBayes, echo=FALSE, message=FALSE, warning=FALSE}
train_control <- trainControl(
  method = "cv", 
  number = 10
)
search_grid <- expand.grid( #tuning parameters
  usekernel = c(TRUE, FALSE),
  laplace = 0:5,
  adjust = seq(0, 5, by = 1)
)

# train model
nb.fit.all <- train(  #fits with all variables
  x = x,
  y = y,
  method = "naive_bayes",
  trControl = train_control,
  tuneGrid = search_grid
  )

nb.pred.all <- predict(nb.fit.all, newdata = german_credit_test) #predicts with all vaRiables
nb.cm <- confusionMatrix(nb.pred.all, german_credit_test$credit_response,positive = "good")

#prediction and metrics using all variables
tab.nb <- data_frame(method = "NB", Accuracy =nb.cm$overall["Accuracy"], Sensitivity=data.frame(nb.cm$byClass["Sensitivity"])[1,1],
           Specificity=data.frame(nb.cm$byClass["Specificity"])[1,1]) 

print(knitr::kable(tab.nb))

plot(varImp(nb.fit.all),  main="Naive bayes: variable importance")



#the next code chunk selects the predictors whose importance is >30 
#and creates a new df whose variables are only he five most important 
nb.importance <- varImp(nb.fit.all) %>% .$importance 
nb.importance <- nb.importance%>% mutate(variable=row.names(nb.importance)) %>%  filter(., good >30) %>% .$variable
nb.impIndexes <- names(x) %in% nb.importance 
nb.dataImp <- x[nb.impIndexes]

###re-fitting using variable importance
nb.fit.varimp <- train( # train a model using only the "varimp>30" df
  nb.dataImp,y,
  method = "naive_bayes",
  trControl = train_control,
  tuneGrid = search_grid
  )

#prediction and metrics using all the five most important variables
nb.pred.imp <- predict(nb.fit.varimp, newdata = german_credit_test)
nb.cm.imp <- confusionMatrix(nb.pred.imp, german_credit_test$credit_response,positive = "good")

tab.nb_imp <- data_frame(method = "NB-IMP", Accuracy =nb.cm.imp$overall["Accuracy"], Sensitivity=data.frame(nb.cm.imp$byClass["Sensitivity"])[1,1],
                     Specificity=data.frame(nb.cm.imp$byClass["Specificity"])[1,1]) 



```
 
 The model has been retrained using the most five important predictors computed by varimp: Accuracy is quite the same as the previous "all-variables" model, slightly better sensitivity but an higher FPR
 `r {print(knitr::kable(tab.nb_imp))}`
 
**PRINCIPAL COMPONENT ANALYSIS**

  The idea of PCA is simple: reduce the number of variables of a data set, while preserving as much information as possible. This model is based on a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.
 In this particular case, there are 1.000 observations covered by 19 variables. As seen before, none of them is correlated to each other, so we expect no great advatage using PCA.
 The input dataset has to be numeric and normalized, so this has be done.
 Next, the pca summary shows numerical evidence of quite uniform variability distribution, that is also possible to apprecciate by two plots:  the cumulative one illustrates that variability sums up almost as a stright line trend following a gentle *positive* slope.    

```{r  pcaEval,  echo=FALSE, message=TRUE,warning=TRUE}
# first of all, the matrix must be normalized 
z <- apply(as.matrix(credit_num[,1:19]),2, scale) 

#then apply the PCA function
pca <- prcomp(z) #pc of numeric version of german_credit dataset
pca.summary <- summary(pca)
print(pca.summary)
proportion.variance <- pca.summary$importance[2,] # select vector of  variance proportion by PCx
proportion.cumulative <- pca.summary$importance[3,] #select vector of cumulative proportion  by PCx

#plot the proportions
plot(proportion.cumulative, type="l",xlab="PCs",ylab="Cumulative Proportion",main = "PCA: Cumulative trend")
plot(proportion.variance, type="b",  xlab="PCs",ylab="Variance Proportion",main = "PCA: Variance distribution")

#the three principal components ( PC1,PC2,PC3)
pr_comp <- round(sum(proportion.variance[1:3])*100)



```

The variance distribution shows that beyond the first three component there is a great amount of variability, i.e. the principal three component have only the  `r {print(pr_comp)}`% of total variability. On the basis of this evidence, no training/predictions have been done using PCA.


## Results

Prior to choose the best approach, we should keep always keep in mind the business requirements related to the particular project. In other words, the decision is to choose  the best model to maximize profits and minimize losses for the bank. These would be the main rules to follow:

  • To predict a customer with bad credit rating as good, the bank will end up losing the whole credit amount lent to him since he will default on the payment and so loss for the bank is 100%, so the FPR should be as low as possible ( specificity as high as possible )
  
  • To incorrectly predict a customer with good credit rating as bad, means to deny him the credit loan but there is neither any profit nor any loss involved in this case. High sensitivity is appreciated but is more important good specificity.
  
  • To correctly predict a customer with bad credit rating as bad, means correctly deny
him a credit loan and so there is neither any loss nor any profit. The results as the same as the previous conditions, but there is better accuracy and better prediction skills.

• To correctly predict a customer with good credit rating as good, means, of course, the main target, if the TPR is much more higher than FPR.
  
Considering all the above rules, none of the model so far used and shown in the table would perfectly fit, so we should choose a trade-off approach.

NB and RF get the same AUC shold appear best two, but the first has better specificity ( we have to keep in mind the importance of low FPR) on the other hand RF has much better sensitivity. So in this case, a deep analisys shold be done in order to understand the weight of *credit_amount* ( the most important variable in all methods) and choose for better sensitivity than specificity.


```{r Results,  echo=FALSE, message=FALSE,warning=FALSE}
#the cumulative auc curve for each model
library(pROC)
par(pty="s")
plot.roc(as.ordered(german_credit_test$credit_response),as.ordered(glm_pred),print.auc=TRUE, percent=TRUE,print.auc.y=50,col="red")
plot.roc(as.ordered(german_credit_test$credit_response),as.ordered(rf.pred.all),print.auc=TRUE, percent=TRUE,print.auc.y=45,col="blue", add=TRUE)
plot.roc(as.ordered(german_credit_test$credit_response),as.ordered(rpart.pred.all),print.auc=TRUE, percent=TRUE,print.auc.y=40,col="green", add=TRUE)
plot.roc(as.ordered(german_credit_test$credit_response),as.ordered(nb.pred.all),print.auc=TRUE, percent=TRUE,print.auc.y=35,col="brown", add=TRUE)
legend("bottomright",y.intersp=0.6, legend=c("GLM","RF", "RPART","NB"), col=c("red", "blue","green", "brown"), lwd=2)

#Next code chunk binds all the models results in a one-shot table, and then prints output
tab.complete <- tab.glm
tab.entry <- list(tab.rf,tab.rf_imp,tab.rpart,tab.rpart_imp,tab.nb,tab.nb_imp)
for(tab.record in tab.entry){
  
  tab.complete <- bind_rows(tab.complete,tab.record)
}

print(knitr::kable(tab.complete))

```

## Conclusions


The data scientist should always keep in mind that the business requirement set is the main milestone when he works on whatever project. The techineques adopted in this project would aim to better explore the business rules and  the problem domain and eventually would find out the solution that best fits the use cases. As seen in the results section, none of the models perfectly fits the solution: for example, further algorithms could be used and benchmarked; deeper analysis on important variable could be done. Ensemble tecniques shoold be helpful. Anyway, for this project, to choose NB or RF cold be a satisfactory solution.

